{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "from numba import cuda, float32, float64\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import random\n",
    "from numba import cuda\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\apps\\\\opt\\\\spark-3.0.3-bin-hadoop2.7'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark Application with califorina housing\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "spark = SparkSession.builder.appName(\"PySpark Application with califorina housing\").getOrCreate()\n",
    "print(spark.sparkContext.appName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(file):\n",
    "    file=open(file, 'r')\n",
    "    reader=csv.reader(file)\n",
    "    data= []\n",
    "    for line in reader:\n",
    "        data.append(line)\n",
    "    data=np.array(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computes wT*x[i] ( the predicted value for datum)\n",
    "def H(x,w,i):\n",
    "    sum=0\n",
    "    for j in range(len(x[0])):\n",
    "        sum+=x[i][j]*w[j]\n",
    "    return sum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(x, weights):\n",
    "    weights2=weights[n_inputs*n_hidden:]\n",
    "    z3=0\n",
    "    #Pass data from the initial layer to the hidden neurons\n",
    "    for j in range(0,n_hidden-1):\n",
    "        weights1=weights[n_inputs*j:n_inputs*(j+1)]\n",
    "        result=0\n",
    "        for k in range(0,n_inputs):\n",
    "            result+=(x[k]*weights1[k])\n",
    "            #result+=0.1\n",
    "        #Activation function ReLu\n",
    "        #result=max(0, result)\n",
    "        #We can add what each hidden neuron contributes to the output layer\n",
    "        z3+=result*weights2[j]\n",
    "    #We add the bias\n",
    "    z3+=weights2[n_hidden-1]\n",
    "    return z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for calculating the error\n",
    "def forward_propagation_Error(x,y,w):\n",
    "    E=0\n",
    "    for i in range(0, len(x)):\n",
    "        wx=0\n",
    "        wx=forward_prop(x[i], w)\n",
    "        E+=((wx-y[i][0])**2)\n",
    "    E=E/len(x)\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Particle:\n",
    "    def __init__(self, position, initial_fitness):\n",
    "         # particles position\n",
    "        self.particle_position = position\n",
    "        #initial value of the particle (infinity or minus infinity,\n",
    "        #depending on whether we want to maximize or minimize)\n",
    "        self.fitness_particle_position = initial_fitness\n",
    "        # best position of the particle\n",
    "        self.local_best_particle_position = []  \n",
    "        #best initial value of the particle (infinity or minus infinity, \n",
    "        #depending on whether we want to maximize or minimize)\n",
    "        self.fitness_local_best_particle_position = initial_fitness  \n",
    "        # particle's velocity\n",
    "        self.particle_velocity = []  \n",
    "        for i in range(0,num_dimensions):\n",
    "            #we generate the initial velocity randomly\n",
    "            self.particle_velocity.append(random.uniform(-vMax, vMax))  \n",
    " \n",
    "    def evaluate(self, x, y, mm):\n",
    "        self.fitness_particle_position = forward_propagation_Error(x, y, self.particle_position)\n",
    "        if mm == -1:\n",
    "            if self.fitness_particle_position < self.fitness_local_best_particle_position:\n",
    "                # We update the best local position\n",
    "                self.local_best_particle_position = self.particle_position  \n",
    "                # We update the best local value \n",
    "                self.fitness_local_best_particle_position = self.fitness_particle_position          \n",
    "        if mm == 1:\n",
    "            if self.fitness_particle_position > self.fitness_local_best_particle_position:\n",
    "                 # We update the best local position\n",
    "                self.local_best_particle_position = self.particle_position  \n",
    "                # We update the best local value \n",
    "                self.fitness_local_best_particle_position = self.fitness_particle_position   \n",
    "    def update_velocity(self, global_best_particle_position, w, Vmax, c1=2.8, c2=1.3):\n",
    "        for i in range(0,num_dimensions):\n",
    "            r1 = random.random()\n",
    "            r2 = random.random()\n",
    "            #We calculate the new velocity\n",
    "            self.particle_velocity[i] = w * self.particle_velocity[i] + c1 * r1 * (self.local_best_particle_position[i] - self.particle_position[i]) + c2 * r2 * (global_best_particle_position[i] - self.particle_position[i])\n",
    "            \n",
    "            #We limit the maxmum velocity\n",
    "            if(self.particle_velocity[i]>Vmax):\n",
    "                self.particle_velocity[i]=Vmax\n",
    "            if(self.particle_velocity[i]<-Vmax):\n",
    "                self.particle_velocity[i]=-Vmax\n",
    " \n",
    "    def update_position(self, bounds):\n",
    "        for i in range(0,num_dimensions):\n",
    "            self.particle_position[i] = self.particle_position[i] + self.particle_velocity[i]\n",
    " \n",
    "            #if it reaches the edges, it stays within, it does not exceed the limits\n",
    "            if self.particle_position[i] > bounds[1]:\n",
    "                self.particle_position[i] = bounds[1]\n",
    "            if self.particle_position[i] < bounds[0]:\n",
    "                self.particle_position[i] = bounds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"projectName\").setMaster(\"local[*]\")\n",
    "sc = SparkContext.getOrCreate(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prior removing [['' 'longitude' 'latitude' ... 'median_income' 'median_house_value'\n",
      "  'ocean_proximity']\n",
      " ['0' '-122.23' '37.88' ... '8.3252' '452600.0' 'NEAR BAY']\n",
      " ['1' '-122.22' '37.86' ... '8.3014' '358500.0' 'NEAR BAY']\n",
      " ...\n",
      " ['20637' '-121.22' '39.43' ... '1.7' '92300.0' 'INLAND']\n",
      " ['20638' '-121.32' '39.43' ... '1.8672' '84700.0' 'INLAND']\n",
      " ['20639' '-121.24' '39.37' ... '2.3886' '89400.0' 'INLAND']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PS0 parameters\n",
    "num_iters=900\n",
    "num_particles=50\n",
    "c1=1.0\n",
    "c2=1.2\n",
    "factor=10\n",
    "Fi=c1+c2\n",
    "Xi=2/(abs(2-Fi-math.sqrt(abs(Fi**2-4*Fi))))\n",
    "data=readData('housing_Out.csv')\n",
    "print('Data prior removing',data)\n",
    "#We remove the first row of names and the last column\n",
    "# data=data[1:500,27:33]\n",
    "data=data[1:500,5:10]\n",
    "rdd = sc.parallelize(data)\n",
    "type(rdd)\n",
    "# # df = pandas.DataFrame(data=arr, index=None, columns=None)\n",
    "# # read_data = np.array(read_data, dtype=np.float32)\n",
    "# # df = pd.DataFrame(read_data)\n",
    "# datosPandas=pd.DataFrame(data)\n",
    "# # data = spark.sparkContext.parallelize(read_data)\n",
    "# # df = data.toDF().collect()\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-0b4b42ff0224>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\opt\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mtoDF\u001b[1;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \"\"\"\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\opt\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    603\u001b[0m             return super(SparkSession, self).createDataFrame(\n\u001b[0;32m    604\u001b[0m                 data, schema, samplingRatio, verifySchema)\n\u001b[1;32m--> 605\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_create_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\opt\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 628\u001b[1;33m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    629\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\opt\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[1;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[0;32m    423\u001b[0m         \"\"\"\n\u001b[0;32m    424\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m             \u001b[0mstruct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\opt\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[1;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[0;32m    395\u001b[0m         \"\"\"\n\u001b[0;32m    396\u001b[0m         \u001b[0mfirst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m             raise ValueError(\"The first row in RDD is empty, \"\n\u001b[0;32m    399\u001b[0m                              \"can not infer schema\")\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "df = rdd.toDF()\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#PS0 parameters\n",
    "num_iters=900\n",
    "num_particles=50\n",
    "c1=1.0\n",
    "c2=1.2\n",
    "factor=10\n",
    "Fi=c1+c2\n",
    "Xi=2/(abs(2-Fi-math.sqrt(abs(Fi**2-4*Fi))))\n",
    "data=readData('housing_Out.csv')\n",
    "print('Data prior removing',data)\n",
    "#We remove the first row of names and the last column\n",
    "# data=data[1:500,27:33]\n",
    "data=data[1:500,5:10]\n",
    "#print('Data after removing',data)\n",
    "# We preprocess the data\n",
    "datosPandas=pd.DataFrame(rdd)\n",
    "#We refill the missing data with the averagge\n",
    "datosPandas.replace('?',np.NaN,inplace=True)\n",
    "imp=SimpleImputer(missing_values=np.NaN)\n",
    "datos=imp.fit_transform(datosPandas)\n",
    "#print('Datos after pre-processing',datos)\n",
    "# Shuffle the data list above\n",
    "np.random.shuffle(datos)\n",
    "#Percentage of training (in this sample, 50%) and percentage of testing (25%)\n",
    "p_train = 0.7\n",
    "#Number of elements in the training and test datasets\n",
    "len_train=int((len(datos))*p_train)\n",
    "datos_train=datos[:len_train,:]\n",
    "datos_test=datos[(len_train):,:]\n",
    "#We separate the data \"x\" from the \"y\" (the data \"y\" is in the last column)\n",
    "num_atrib=int(len(datos[0]))-1\n",
    "x_train=datos_train[:,:num_atrib]\n",
    "y_train=datos_train[:,num_atrib]\n",
    "x_test=datos_test[:,:num_atrib]\n",
    "y_test=datos_test[:,num_atrib]\n",
    "# datosPandas.head()\n",
    "# print('vector x_train ',x_train)\n",
    "# print('vector y_train ',y_train)\n",
    "# df = pd.DataFrame(data)\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.sparkContext.parallelize(datos)\n",
    "display(data.collect())\n",
    "df = data.toDF().collect()\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data re-scaling\n",
    "y_train=np.reshape(y_train, (-1,1))\n",
    "y_train.reshape(-1,1)\n",
    "y_test=np.reshape(y_test, (-1,1))\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "#\n",
    "scaler_x.fit(x_train)\n",
    "x_train=scaler_x.transform(x_train)\n",
    "x_test=scaler_x.transform(x_test)\n",
    "#\n",
    "scaler_y.fit(y_train)\n",
    "y_train=scaler_y.transform(y_train)\n",
    "y_test=scaler_y.transform(y_test)\n",
    "print('vector x_train scaled ',x_train)\n",
    "print('vector y_train scaled',y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural network parameters (as function of the input)\n",
    "n_inputs=x_train[0].size\n",
    "n_hidden=math.floor(len(x_train)/(factor*(n_inputs+1)))+1\n",
    "#We calculate the number of weights necessary to carry out the computation\n",
    "num_weights=(n_inputs*n_hidden)+n_hidden\n",
    "bound0=float(max(y_train))#upper limit\n",
    "bound1=-bound0 #lower limit\n",
    "vMax=bound0*0.6\n",
    "weights=[]\n",
    "for i in range(0,num_particles):\n",
    "        for j in range(0, num_weights):\n",
    "            weights.append(random.uniform(-1, 1))\n",
    "#print('total number of weights ', len(weights))\n",
    "#print('Initial weights values', weights )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "    # the parameters are the data, the number of neurons of the input layer, \n",
    "    # the number of neurons of the hidden layer and \n",
    "    # the number of neurons of the output layer\n",
    "    def __init__(self, x, y, n_inputs, n_hidden, weights, num_weights):\n",
    "        self.input=x\n",
    "        self.y=y\n",
    "        \n",
    "        self.weights=weights\n",
    "        self.num_weights=num_weights\n",
    "        \n",
    "    def train_PSO(self):\n",
    "        best_position, fitness=PSO_Neural_Network(self.input, self.y,self.weights, self.num_weights,[bound1,bound0], -1, num_particles, 100, 0.9, 0.1, vMax)\n",
    "        self.weights=best_position\n",
    "        return fitness\n",
    "    \n",
    "    def predict(self, x_test):\n",
    "        return forward_prop(x_test, self.weights)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PSO parallel\n",
    "@cuda.jit\n",
    "#def PSO_GPU(x, y):\n",
    "def PSO_GPU(x, y, particle_positions, particle_velocity, personal_best_particle_position,fitness_personal_best_particle_position, local_best_particle_position, fitness_local_best_particle_position,pool, c1, c2, Vmax, bound0, bound1):\n",
    "  \n",
    "    particle= cuda.blockIdx.x #We found the particle that corresponds to the block where the thread is\n",
    "    nx=cuda.threadIdx.x #Finds the neurons of the hidden layer that this thread is going to calculate \n",
    "    #in this thread. Thread '0' does the rest of the particle calculations\n",
    "   \n",
    "    #We initialize the vectors in a shared-memory array (common to the block == to the particle)\n",
    "    z2=cuda.shared.array(shape=n_hidden, dtype=float32)\n",
    "\n",
    "    for iters in range(0, num_iters):\n",
    "        #We take particle's position corresponding to the thread\n",
    "        position=particle_positions[num_weights*particle: num_weights*(particle+1)]\n",
    "\n",
    "        #We do forward propagation\n",
    "        fitness_position=0\n",
    "        weights2=position[n_inputs*n_hidden:]\n",
    "        for i in range(0, len(x)):\n",
    "            j=nx\n",
    "            while(j<n_hidden):\n",
    "                #Pass from the init layer to the hidden layer\n",
    "                hidden_result=0\n",
    "                weights1=position[n_inputs*j:n_inputs*(j+1)]\n",
    "                for k in range(0,n_inputs-1):\n",
    "                    hidden_result+=(x[i][k]*weights1[k])\n",
    "                hidden_result+=weights1[n_inputs-1] #We add the bias\n",
    "                #Function ReLu (activation of)\n",
    "                hidden_result=max(0, hidden_result)\n",
    "                #We save the value in the vector z2, to add it when the total value \n",
    "                #is calculated in the output layer\n",
    "                z2[j]=hidden_result*weights2[j]\n",
    "                j=j+threadsperblock\n",
    "   \n",
    "            # Thread synchronization\n",
    "            cuda.syncthreads()\n",
    "           \n",
    "            #Thread '0' gathers the results of other threads\n",
    "            if(nx==0):\n",
    "                result=0\n",
    "                for h in range (0, n_hidden):\n",
    "                    result+=z2[h]\n",
    "                fitness_position+=float((y[i][0]-result)**2)\n",
    "            # Thread synchronization\n",
    "            cuda.syncthreads()\n",
    "            \n",
    "        if(nx==0):\n",
    "            #We update the best personal position\n",
    "            fitness_position=fitness_position/len(x)\n",
    "            if fitness_position < fitness_personal_best_particle_position[particle]:\n",
    "                for i in range(0, num_weights):\n",
    "                    personal_best_particle_position[(num_weights*particle)+i] = particle_positions[num_weights*particle+i]  # we update the best personal position\n",
    "                # we update the best personal value\n",
    "                fitness_personal_best_particle_position[particle] = fitness_position \n",
    "       \n",
    "        # We synchronize the threads\n",
    "        cuda.syncthreads()\n",
    "       \n",
    "        if(nx==0):\n",
    "            #Best local position updating\n",
    "            if fitness_personal_best_particle_position[particle]< fitness_local_best_particle_position[particle]:\n",
    "                for i in range(0, num_weights):\n",
    "                    local_best_particle_position[num_weights*particle+i] = personal_best_particle_position[(num_weights*particle)+i]\n",
    "                fitness_local_best_particle_position[particle] = fitness_personal_best_particle_position[particle]\n",
    "           \n",
    "            if fitness_personal_best_particle_position[int(math.fmod((num_particles+particle-1), num_particles))] < fitness_local_best_particle_position[particle]:\n",
    "                for i in range(0, num_weights):\n",
    "                    local_best_particle_position[num_weights*(particle)+i] = particle_positions[num_weights*(int(math.fmod((num_particles+particle-1), num_particles)))+i]\n",
    "                fitness_local_best_particle_position[particle] = fitness_personal_best_particle_position[int(math.fmod((num_particles+particle-1), num_particles))]\n",
    "           \n",
    "               \n",
    "            if fitness_personal_best_particle_position[int(math.fmod((num_particles+particle+1), num_particles))] < fitness_local_best_particle_position[particle]:\n",
    "                for i in range(0, num_weights):\n",
    "                    local_best_particle_position[num_weights*(particle)+i] = particle_positions[num_weights*(int(math.fmod((num_particles+particle+1), num_particles)))+i]\n",
    "                fitness_local_best_particle_position[particle] = fitness_personal_best_particle_position[int(math.fmod((num_particles+particle+1), num_particles))]\n",
    "           \n",
    "        # Thread synchronization\n",
    "        cuda.syncthreads()\n",
    "        \n",
    "        #We choose numbers from the random numbers repository\n",
    "        r1=pool[2*num_weights*num_particles+particle+iters]\n",
    "        r2=pool[2*num_weights*num_particles-particle-iters]\n",
    "       \n",
    "        i=nx\n",
    "        #We update each particle's velocity\n",
    "        while i<num_weights:\n",
    "            #We calculate the new velocity\n",
    "            particle_velocity[num_weights*particle+i] = Xi*( particle_velocity[num_weights*particle+i] + c1 * r1 * (personal_best_particle_position[num_weights*particle+i] - particle_positions[num_weights*particle+i]) + c2 * r2 * (local_best_particle_position[num_weights*particle+i] - particle_positions[num_weights*particle+i]))\n",
    "            #We limit speed to maximum speed\n",
    "            if particle_velocity[num_weights*particle+i]>Vmax :\n",
    "                particle_velocity[num_weights*particle+i]=Vmax\n",
    "            if particle_velocity[num_weights*particle+i]<(Vmax*(-1)):\n",
    "                particle_velocity[num_weights*particle+i]=(Vmax*(-1))\n",
    "            i+=threadsperblock\n",
    "       \n",
    "        # Wait until all threads finish computing\n",
    "        cuda.syncthreads()      \n",
    "        i=nx\n",
    "        #We update each particle's position\n",
    "        while i<num_weights:\n",
    "            particle_positions[num_weights*particle+i] = particle_positions[num_weights*particle+i] + particle_velocity[num_weights*particle+i]\n",
    "            #if it reaches and edge, it stays in there, it does not trespasses it\n",
    "            if particle_positions[num_weights*particle+i] > bound0:\n",
    "                particle_positions[num_weights*particle+i] = bound0\n",
    "            if particle_positions[num_weights*particle+i] < bound1:\n",
    "                particle_positions[num_weights*particle+i] = bound1\n",
    "            i+=threadsperblock\n",
    "           \n",
    "        # Wait until all threads finish computing\n",
    "        cuda.syncthreads()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coresPerSM():\n",
    "    cc_cores_per_SM_dict = {\n",
    "    (2,0) : 32,\n",
    "    (2,1) : 48,\n",
    "    (3,0) : 192,\n",
    "    (3,5) : 192,\n",
    "    (3,7) : 192,\n",
    "    (5,0) : 128,\n",
    "    (5,2) : 128,\n",
    "    (6,0) : 64,\n",
    "    (6,1) : 128,\n",
    "    (7,0) : 64,\n",
    "    (7,5) : 64,\n",
    "    (8,0) : 64,\n",
    "    (8,6) : 128\n",
    "    }\n",
    "    # the above dictionary should result in a value of \"None\" if a cc match \n",
    "    # is not found.  The dictionary needs to be extended as new devices become\n",
    "    # available, and currently does not account for all Jetson devices\n",
    "    device = cuda.get_current_device()\n",
    "    #my_cc = getattr(device, 'COMPUTE_CAPABILITY')\n",
    "    my_cc=(6,1)\n",
    "    cores_per_sm = cc_cores_per_SM_dict.get(my_cc)\n",
    "    return cores_per_sm\n",
    "device = cuda.get_current_device()\n",
    "threadsperblock =  coresPerSM()\n",
    "blockspergrid=num_particles\n",
    "print('device, num weights, threads per block, blocks, c1, c2, vMax, bound0, bound1 ->',getattr(device, 'MULTIPROCESSOR_COUNT'),num_weights,threadsperblock,blockspergrid,c1, c2, vMax, bound0, bound1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We generate one pool of random numbers and initial data\n",
    "pool=[]\n",
    "fitness_personal_best_particle_position=[]\n",
    "fitness_local_best_particle_position=[]\n",
    "for i in range(0,num_particles):\n",
    "        for j in range(0, num_weights*3):\n",
    "            pool.append(random.uniform(bound1, bound0))\n",
    "        fitness_local_best_particle_position.append(float(\"inf\"))\n",
    "        fitness_personal_best_particle_position.append(float(\"inf\"))\n",
    "#We load data to the gpu\n",
    "x_global_mem=cuda.to_device(np.ascontiguousarray(x_train))\n",
    "y_global_mem=cuda.to_device(np.ascontiguousarray(y_train))\n",
    "fitness_personal_best_particle_position_global_mem=cuda.to_device(np.asarray(fitness_personal_best_particle_position))\n",
    "personal_best_particle_position=cuda.to_device(np.zeros(num_weights*num_particles))\n",
    "local_best_particle_position=cuda.to_device(np.zeros(num_weights*num_particles))\n",
    "fitness_local_best_particle_position_global_mem=cuda.to_device(np.ascontiguousarray(fitness_local_best_particle_position))\n",
    "pool_global_mem=cuda.to_device(np.ascontiguousarray(pool))\n",
    "\n",
    "#We initiate positions and velocities of particles by taking data from the pool of random numbers\n",
    "particles_positions=pool_global_mem[0:num_weights*num_particles]\n",
    "particles_velocity=pool_global_mem[num_weights*num_particles:2*num_weights*num_particles]\n",
    "print(\"Data loaded in GPU correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "gpu = cuda.get_current_device()\n",
    "print(\"name = %s\" % gpu.name)\n",
    "print(\"maxThreadsPerBlock = %s\" % str(gpu.MAX_THREADS_PER_BLOCK))\n",
    "print(\"maxBlockDimX = %s\" % str(gpu.MAX_BLOCK_DIM_X))\n",
    "print(\"maxBlockDimY = %s\" % str(gpu.MAX_BLOCK_DIM_Y))\n",
    "print(\"maxBlockDimZ = %s\" % str(gpu.MAX_BLOCK_DIM_Z))\n",
    "print(\"maxGridDimX = %s\" % str(gpu.MAX_GRID_DIM_X))\n",
    "print(\"maxGridDimY = %s\" % str(gpu.MAX_GRID_DIM_Y))\n",
    "print(\"maxGridDimZ = %s\" % str(gpu.MAX_GRID_DIM_Z))\n",
    "print(\"maxSharedMemoryPerBlock = %s\" % \n",
    "str(gpu.MAX_SHARED_MEMORY_PER_BLOCK))\n",
    "print(\"asyncEngineCount = %s\" % str(gpu.ASYNC_ENGINE_COUNT))\n",
    "print(\"canMapHostMemory = %s\" % str(gpu.CAN_MAP_HOST_MEMORY))\n",
    "print(\"multiProcessorCount = %s\" % str(gpu.MULTIPROCESSOR_COUNT))\n",
    "print(\"warpSize = %s\" % str(gpu.WARP_SIZE))\n",
    "print(\"unifiedAddressing = %s\" % str(gpu.UNIFIED_ADDRESSING))\n",
    "print(\"pciBusID = %s\" % str(gpu.PCI_BUS_ID))\n",
    "print(\"pciDeviceID = %s\" % str(gpu.PCI_DEVICE_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And now it's time to launch the computation in parallel!!!!\n",
    "start = time.time()\n",
    "cuda.profile_start()\n",
    "\n",
    "PSO_GPU[blockspergrid, threadsperblock](x_global_mem, y_global_mem,particles_positions, particles_velocity, personal_best_particle_position,fitness_personal_best_particle_position_global_mem, local_best_particle_position,fitness_local_best_particle_position_global_mem, pool_global_mem,c1, c2, vMax, bound0, bound1)\n",
    "\n",
    "local_best_particle_position_host=local_best_particle_position.copy_to_host()\n",
    "fitness_local_best_particle_position=fitness_local_best_particle_position_global_mem.copy_to_host()\n",
    "cuda.profile_stop()\n",
    "end = time.time()\n",
    "time=end - start\n",
    "print(\"Execution time(s): \", time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "global_best_particle_position=[]\n",
    "fitness_global_best_particle_position=float(\"inf\")\n",
    "for i in range(0, num_particles):\n",
    "    if(fitness_local_best_particle_position[i]<fitness_global_best_particle_position):\n",
    "        global_best_particle_position=local_best_particle_position_host[num_weights*i:num_weights*(i+1)]\n",
    "        fitness_global_best_particle_position=fitness_local_best_particle_position[i]\n",
    "\n",
    "#We calculate the transfer time between the host (CPU) and the device (GPU), and we remove it from the calculation\n",
    "start2 = time.time()\n",
    "local_best_particle_position_host=local_best_particle_position.copy_to_host()\n",
    "fitness_local_best_particle_position=fitness_local_best_particle_position_global_mem.copy_to_host()\n",
    "end2 = time.time()\n",
    "time_corr=end - start -(end2-start2)\n",
    "#Error calculation\n",
    "Eout=0\n",
    "Error_Cero=0\n",
    "prediction=np.zeros(len(x_test))\n",
    "for i in range(0, len(x_test)):\n",
    "        prediction[i]=forward_prop(x_test[i], global_best_particle_position)\n",
    "        # mean standard error (MSE), for regression problems\n",
    "        Eout+=float((y_test[i][0]-prediction[i]))**2\n",
    "Eout=Eout/len(x_test)\n",
    "#results\n",
    "print(\"Number of iterations of PSO =\", num_iters)\n",
    "print(\"Running time(s): \", time_corr)\n",
    "print('Ein:', fitness_global_best_particle_position)\n",
    "print('Eout:', Eout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction=np.zeros(len(x_test))\n",
    "red_neuronal=Neural_Network(x_train, y_train, n_inputs, n_hidden, weights, num_weights)\n",
    "\n",
    "# red_neuronal.fit(x_train,y_train)\n",
    "pred_results = []\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    y_pred=red_neuronal.predict(x_test[i])\n",
    "    y_pred=(1/len(x_test))*((y_pred-y_test[i])**2)\n",
    "    pred_results.append(y_pred)\n",
    "np.hstack(pred_results)\n",
    "pred_results = np.array(pred_results, dtype=np.float32)\n",
    "maxValue_pred_results = np.max(pred_results)\n",
    "minValue_pred_results = np.min(pred_results)\n",
    "median_pred_results = np.median(pred_results)\n",
    "median_y_test = np.median(y_test)\n",
    "maxValue_y_test = np.max(y_test)\n",
    "minValue_y_test = np.min(y_test)\n",
    "print('median_pred_results: ', median_pred_results)\n",
    "print('max_pred_results: ', maxValue_pred_results)\n",
    "print('min_pred_results: ',minValue_pred_results)\n",
    "print('median_y_test: ', median_y_test)\n",
    "print('max_y_test: ',maxValue_y_test)\n",
    "print('min_y_test: ',minValue_y_test)\n",
    "print('pred_results: \\n', pred_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_predict = 0.002                       # decide on a cutoff limit\n",
    "y_pred_classes = np.zeros_like(pred_results)    # initialise a matrix full with zeros\n",
    "y_pred_classes[pred_results > threshold_predict] = 1  \n",
    "threshold_y_test = 0.2\n",
    "# add a 1 if the cutoff was breached\n",
    "y_test_classes = np.zeros_like(y_test)\n",
    "y_test_classes[y_test > threshold_y_test] = 1\n",
    "print('y_pred_classes: \\n ',y_pred_classes.shape)\n",
    "print('y_test_classes: \\n',y_test_classes.shape)\n",
    "print('y_pred_classes: \\n ',y_pred_classes)\n",
    "print('y_test: \\n',y_test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import  confusion_matrix, mean_squared_error, accuracy_score, precision_score, recall_score, f1_score\n",
    "mean_squared_error(y_test_classes,pred_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax=plt.subplots(figsize=(5,5))\n",
    "cm=confusion_matrix(y_test_classes, y_pred_classes)\n",
    "sns.heatmap(cm,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "plt.xlabel(\"pred_results\")\n",
    "plt.ylabel(\"y_test\")\n",
    "plt.show()\n",
    "print('Confusion matrix: \\n',cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print('Classification report: \\n',classification_report(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print('Classification report: \\n',classification_report(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "auc_roc=metrics.roc_auc_score(y_test_classes, y_pred_classes)\n",
    "auc_roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test_classes, y_pred_classes)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "print('roc_auc: ', roc_auc)\n",
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],linestyle='--')\n",
    "plt.axis('tight')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# prediccionClasification=np.zeros(len(y_test_classes))\n",
    "thresholds=[]\n",
    "accuracy=[]\n",
    "precision=[]\n",
    "recall=[]\n",
    "f1=[]\n",
    "\n",
    "# predictionClasification=np.zeros(len(y_test_classes))\n",
    "for i in range(0, len(y_pred_classes)):\n",
    "    accuracy.append(accuracy_score(y_test_classes, y_pred_classes))\n",
    "    precision.append(precision_score(y_test_classes, y_pred_classes))\n",
    "    recall.append(recall_score(y_test_classes, y_pred_classes))\n",
    "    f1.append(f1_score(y_test_classes, y_pred_classes))\n",
    "print('accuracy_score: ', accuracy_score(y_test_classes, y_pred_classes))\n",
    "print('precision_score: ', precision_score(y_test_classes, y_pred_classes))\n",
    "print('recall_score: ', recall_score(y_test_classes, y_pred_classes))\n",
    "print('f1_score: ', f1_score(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc, precision_recall_curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test_classes, y_pred_classes)\n",
    "#create precision-recall curve\n",
    "\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.plot([0, 1], [0.5, 0.5],'--')\n",
    "plt.plot(recall, precision, label = 'Precision')\n",
    "plt.xlabel('recall')\n",
    "plt.ylabel('precision')\n",
    "plt.title('Recall-Precision-curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, r2_score\n",
    "f1= f1_score(y_test_classes, y_pred_classes)\n",
    "print(\"MSE: \",mean_squared_error(y_test_classes,y_pred_classes))\n",
    "print('F1-score: ', f1)\n",
    "print(\"R2: \",r2_score(y_test_classes,y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
